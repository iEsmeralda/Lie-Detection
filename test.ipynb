{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27477cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿CUDA disponible?: True\n",
      "Número de GPUs: 1\n",
      "Nombre GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"¿CUDA disponible?:\", torch.cuda.is_available())\n",
    "print(\"Número de GPUs:\", torch.cuda.device_count())\n",
    "print(\"Nombre GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc84f752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting av\n",
      "  Downloading av-14.4.0-cp311-cp311-win_amd64.whl.metadata (4.7 kB)\n",
      "Downloading av-14.4.0-cp311-cp311-win_amd64.whl (27.9 MB)\n",
      "   ---------------------------------------- 0.0/27.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/27.9 MB 10.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.7/27.9 MB 10.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 6.0/27.9 MB 11.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 8.7/27.9 MB 11.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 11.0/27.9 MB 11.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 13.6/27.9 MB 11.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 16.0/27.9 MB 11.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 18.6/27.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 21.0/27.9 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 23.3/27.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.0/27.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/27.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 27.9/27.9 MB 11.3 MB/s eta 0:00:00\n",
      "Installing collected packages: av\n",
      "Successfully installed av-14.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787ab28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bodoque\\Desktop\\TT\\Lie-Detection\\v_environment\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bodoque\\Desktop\\TT\\Lie-Detection\\v_environment\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R2Plus1D_18_Weights.KINETICS400_V1`. You can also use `weights=R2Plus1D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Bodoque\\Desktop\\TT\\Lie-Detection\\v_environment\\Lib\\site-packages\\torchvision\\io\\_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 — Loss: 0.6188 — Acc: 62.9%\n",
      "Epoch 2/10 — Loss: 0.4363 — Acc: 77.1%\n",
      "Epoch 3/10 — Loss: 0.3225 — Acc: 85.6%\n",
      "Epoch 4/10 — Loss: 0.2628 — Acc: 89.2%\n",
      "Epoch 5/10 — Loss: 0.2361 — Acc: 89.8%\n",
      "Epoch 6/10 — Loss: 0.2166 — Acc: 91.5%\n",
      "Epoch 7/10 — Loss: 0.1274 — Acc: 96.6%\n",
      "Epoch 8/10 — Loss: 0.1264 — Acc: 94.6%\n",
      "Epoch 9/10 — Loss: 0.1142 — Acc: 96.3%\n",
      "Epoch 10/10 — Loss: 0.0951 — Acc: 97.2%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_video\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 1) CONFIGURACIÓN\n",
    "# ——————————————————————————————\n",
    "INPUT_DIR    = os.path.join(\"..\", \"clips_prueba\")\n",
    "FPS          = 30\n",
    "CLIP_FRAMES  = 32    # cada clip tiene 32 frames (~1s)\n",
    "BATCH_SIZE   = 4\n",
    "NUM_EPOCHS   = 10\n",
    "LEARNING_RATE= 1e-4\n",
    "NUM_CLASSES  = 2     # 0 = verdad, 1 = mentira\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 2) PREPARAR RUTAS Y ETIQUETAS\n",
    "# ——————————————————————————————\n",
    "# Asume que dentro de INPUT_DIR tienes dos subcarpetas:\n",
    "truth_dir = os.path.join(INPUT_DIR, \"verdad\")\n",
    "lie_dir   = os.path.join(INPUT_DIR, \"mentira\")\n",
    "\n",
    "truth_paths = [os.path.join(truth_dir, f)\n",
    "               for f in os.listdir(truth_dir) if f.endswith(\".mp4\")]\n",
    "lie_paths   = [os.path.join(lie_dir,   f)\n",
    "               for f in os.listdir(lie_dir)   if f.endswith(\".mp4\")]\n",
    "\n",
    "clip_paths = truth_paths + lie_paths\n",
    "labels     = [0]*len(truth_paths) + [1]*len(lie_paths)\n",
    "\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 3) DATASET + DATALOADER\n",
    "# ——————————————————————————————\n",
    "class ClipDataset(Dataset):\n",
    "    def __init__(self, paths, labels, clip_len=CLIP_FRAMES):\n",
    "        self.paths     = paths\n",
    "        self.labels    = labels\n",
    "        self.clip_len  = clip_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        video, _, _ = read_video(path, pts_unit=\"sec\")  \n",
    "        # video: [T, H, W, C]\n",
    "        # recortamos/pad por si sobran o faltan frames\n",
    "        frames = video[:self.clip_len]  \n",
    "        if frames.shape[0] < self.clip_len:\n",
    "            pad_len = self.clip_len - frames.shape[0]\n",
    "            pad = torch.zeros((pad_len, *frames.shape[1:]), dtype=frames.dtype)\n",
    "            frames = torch.cat([frames, pad], dim=0)\n",
    "\n",
    "        # pasamos a [C, T, H, W] y normalizamos\n",
    "        frames = frames.permute(3,0,1,2).float() / 255.0\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return frames, label\n",
    "\n",
    "dataset   = ClipDataset(clip_paths, labels)\n",
    "dataloader= DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 4) MODELO (R(2+1)D pre-entrenado)\n",
    "# ——————————————————————————————\n",
    "model = r2plus1d_18(pretrained=True)\n",
    "in_feat = model.fc.in_features\n",
    "model.fc = nn.Linear(in_feat, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 5) CRITERIO Y OPTIMIZADOR\n",
    "# ——————————————————————————————\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# ——————————————————————————————\n",
    "# 6) BUCLE DE ENTRENAMIENTO\n",
    "# ——————————————————————————————\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total   = 0\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)             # [B, 2]\n",
    "        loss    = criterion(outputs,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct   += preds.eq(targets).sum().item()\n",
    "        total     += targets.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = 100 * correct / total\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} — \"\n",
    "          f\"Loss: {epoch_loss:.4f} — Acc: {epoch_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a72a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en r2plus1d_lie_detector.pth\n"
     ]
    }
   ],
   "source": [
    "# Después de terminar todos los epochs\n",
    "save_path = \"r2plus1d_lie_detector.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Modelo guardado en {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631bece9",
   "metadata": {},
   "source": [
    "# Para cargar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17bad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Reconstruyes la arquitectura igual que antes\n",
    "model = r2plus1d_18(pretrained=False)       # o True, según necesites\n",
    "in_feat = model.fc.in_features\n",
    "model.fc = nn.Linear(in_feat, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# 2) Cargas los pesos\n",
    "checkpoint = torch.load(\"r2plus1d_lie_detector.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()   # muy importante si luego vas a inferir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
